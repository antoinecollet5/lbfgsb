{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5f976d",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "## üöÄ Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac22607",
   "metadata": {},
   "source": [
    "To install `lbfgsb`, the easiest way is through `pip`:\n",
    "\n",
    "```bash\n",
    "    pip install lbfgsb\n",
    "```\n",
    "Or alternatively using `conda`\n",
    "\n",
    "```bash\n",
    "    conda install lbfgsb\n",
    "```\n",
    "\n",
    "You might also clone the repository and install from source\n",
    "\n",
    "```bash\n",
    "    pip install -e .\n",
    "```\n",
    "\n",
    "Once the installation is done, given an optimization problem defined by an objective function and a feasible space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd29ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lbfgsb.types import NDArrayFloat  # for type hints, numpy array of floats\n",
    "\n",
    "\n",
    "def rosenbrock(x: NDArrayFloat) -> float:\n",
    "    \"\"\"\n",
    "    The Rosenbrock function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "    Array of of points at which the Rosenbrock function is to be computed.\n",
    "    It can be of shape (m,) or (m, n), m being the number of variables per vector\n",
    "    of parameters and n the number of different vectors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The gradient of the Rosenbrock function with size (n,).\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    sum1 = ((x[1:] - x[:-1] ** 2.0) ** 2.0).sum(axis=0)\n",
    "    sum2 = np.square(1.0 - x[:-1]).sum(axis=0)\n",
    "    return 100.0 * sum1 + sum2\n",
    "\n",
    "\n",
    "def rosenbrock_grad(x: NDArrayFloat) -> NDArrayFloat:\n",
    "    \"\"\"\n",
    "    The gradient of the Rosenbrock function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "    Array of of points at which the Rosenbrock function is to be computed.\n",
    "    It can be of shape (m,) or (m, n), m being the number of variables per vector\n",
    "    of parameters and n the number of different vectors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NDArrayFloat\n",
    "        The gradient(s) of the Rosenbrock function with the same shapes as the input x.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    g = np.zeros(x.shape)\n",
    "    # derivation of sum1\n",
    "    g[1:] += 100.0 * (2.0 * x[1:] - 2.0 * x[:-1] ** 2.0)\n",
    "    g[:-1] += 100.0 * (-4.0 * x[1:] * x[:-1] + 4.0 * x[:-1] ** 3.0)\n",
    "    # derivation of sum2\n",
    "    g[:-1] += 2.0 * (x[:-1] - 1.0)\n",
    "    return g\n",
    "\n",
    "\n",
    "lb = np.array([-2, -2])  # lower bounds\n",
    "ub = np.array([2, 2])  # upper bounds\n",
    "bounds = np.array((lb, ub)).T  # The number of variables to optimize is len(bounds)\n",
    "x0 = np.array([-0.8, -1])  # The initial guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41015ee",
   "metadata": {},
   "source": [
    "The optimal solution can be found following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26e13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbfgsb import minimize_lbfgsb\n",
    "\n",
    "res = minimize_lbfgsb(\n",
    "    x0=x0, fun=rosenbrock, jac=rosenbrock_grad, bounds=bounds, ftol=1e-5, gtol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4fd17",
   "metadata": {},
   "source": [
    "``minimize_lbfgsb`` returns an `OptimalResult` instance (from [Scipy](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html)) that contains the results of the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3019ae2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 4.591223585995676e-09\n",
       "        x: [ 1.000e+00  9.999e-01]\n",
       "      nit: 17\n",
       "      jac: [ 1.789e-03 -9.432e-04]\n",
       "     nfev: 21\n",
       "     njev: 21\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b543c",
   "metadata": {},
   "source": [
    "Note that unlike `minimize` from scipy, `minimize_lbfgsb` does not accept `args` nor `kwargs`. Hence you will have to define\n",
    "wrappers if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f56419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cannot be passed to minimize_lbfgsb\n",
    "def my_cost_function(\n",
    "    x: NDArrayFloat,\n",
    "    arg1: int,\n",
    "    arg2: float,\n",
    "    *,\n",
    "    kwargs1: int = 0,\n",
    "    kwargs2: str = \"blabla\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Return a float and takes args and kwargs.\n",
    "    \"\"\"\n",
    "    return 5657.0  # just do something and return a float\n",
    "\n",
    "\n",
    "# This can be passed to minimize_lbfgsb\n",
    "def my_wrapper(x: NDArrayFloat) -> float:\n",
    "    \"\"\"\n",
    "    Return a float and takes args and kwargs.\n",
    "    \"\"\"\n",
    "    return my_cost_function(x, 10, 239.9, kwargs1=1, kwargs2=\"blabla2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae7d5a",
   "metadata": {},
   "source": [
    "See all use cases in the tutorials section of the [documentation](https://lbfgsb.readthedocs.io/en/latest/usage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ae4bb",
   "metadata": {},
   "source": [
    "## ‚ö° Performance\n",
    "\n",
    "Although memory usage and runtime remain reasonable thanks to NumPy and extensive vectorization, a pure Python implementation is inherently slower and more memory-intensive than the SciPy reference implementation. The latter is written in low-level languages\n",
    "([originally Fortran 77 and, since SciPy v1.15.0, ported to C](https://docs.scipy.org/doc/scipy/release/1.15.0-notes.html#scipy-optimize-improvements>)) and therefore benefits from decades of compiler and library-level optimizations.\n",
    "\n",
    "In practice, this performance gap is negligible for small- to medium-scale problems, or when the overall runtime is dominated by evaluations of the objective function and its gradient rather than by the optimization routine itself.\n",
    "\n",
    "To mitigate the overhead of Python in performance-critical sections, we provide a Numba JIT-compiled implementation of the most expensive components of the algorithm. This approach significantly reduces Python overhead and brings performance close to\n",
    "that of [Scipy](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html) for large-scale problems (2 fold speed up for 1M parameters), while still preserving the additional flexibility and features offered by our implementation. The only overhead introduced is a one-time LLVM compilation during the first call; subsequent calls benefit from Numba‚Äôs caching mechanism.\n",
    "\n",
    "Numba acceleration is enabled by default and can be disabled via the boolean parameter `is_use_numba_jit`. If this option is set to True but Numba is not available, a warning is issued and the code automatically falls back to the non-JIT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99158f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FTOL\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 4.591223594371224e-09\n",
       "        x: [ 1.000e+00  9.999e-01]\n",
       "      nit: 17\n",
       "      jac: [ 1.789e-03 -9.432e-04]\n",
       "     nfev: 21\n",
       "     njev: 21\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize_lbfgsb(\n",
    "    x0=x0,\n",
    "    fun=rosenbrock,\n",
    "    jac=rosenbrock_grad,\n",
    "    bounds=bounds,\n",
    "    ftol=1e-5,\n",
    "    gtol=1e-5,\n",
    "    is_use_numba_jit=False,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05a816",
   "metadata": {},
   "source": [
    "Note that numba comes as an optional dependency and should be installed in one of the following ways:\n",
    "\n",
    "```bash\n",
    "    pip install lbfgsb[numba]\n",
    "```\n",
    "\n",
    "```bash\n",
    "    pip install lbfgsb numba\n",
    "```\n",
    "\n",
    "Or alternatively using conda\n",
    "\n",
    "```bash\n",
    "    conda install lbfgsb[numba]\n",
    "```\n",
    "\n",
    "```bash\n",
    "    conda install lbfgsb numba\n",
    "```\n",
    "\n",
    "If numba is not found in your environement, a RunTime warning will be raised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccd908",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Unique features\n",
    "\n",
    "Here are some of the unique features that this implementation provides (to the best of our knowledge in 2025).\n",
    "\n",
    "### ‚ú® Checkpointing\n",
    "\n",
    "In quasi-Newtons (L-BFGS-B is one of them), the inverse of the Hessian is approximated from the series of the (`m` last) past gradients. If the optimization is stopped, the history is lost and restarting the optimization would results in a slower convergence (more total objective function and gradient calls) because the inverse Hessian would be reinitiated as the identity.\n",
    "\n",
    "Let's take the previous example with the rosenbrock objective function. But this time, the process is stopped after three calls of the function (`maxfun=3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7b3190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT\n",
      "  success: True\n",
      "   status: 1\n",
      "      fun: 2.233512963961484\n",
      "        x: [ 5.682e-01  4.659e-01]\n",
      "      nit: 1\n",
      "      jac: [-3.338e+01  2.862e+01]\n",
      "     nfev: 3\n",
      "     njev: 3\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "res_3_fun = minimize_lbfgsb(\n",
    "    x0=x0,\n",
    "    fun=rosenbrock,\n",
    "    jac=rosenbrock_grad,\n",
    "    bounds=bounds,\n",
    "    ftol=1e-5,\n",
    "    gtol=1e-5,\n",
    "    maxfun=3,\n",
    ")\n",
    "print(res_3_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da1154",
   "metadata": {},
   "source": [
    "Resuming the minimization from the previous parameters state (`x0=res_3_fun.x`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a7827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 2.0057954719712695e-09\n",
      "        x: [ 1.000e+00  1.000e+00]\n",
      "      nit: 16\n",
      "      jac: [ 1.523e-03 -7.833e-04]\n",
      "     nfev: 23\n",
      "     njev: 23\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "res_final = minimize_lbfgsb(\n",
    "    x0=res_3_fun.x,\n",
    "    fun=rosenbrock,\n",
    "    jac=rosenbrock_grad,\n",
    "    bounds=bounds,\n",
    "    ftol=1e-5,\n",
    "    gtol=1e-5,\n",
    ")\n",
    "print(res_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c99d49",
   "metadata": {},
   "source": [
    "One can see that the total number of calls to the objective function and to its gradient is higher (`3+21 = 24` vs `19` originally). In addition, one needs to compute it manually because it looses track of the state when restarting `L-BFGS-B`. Also one sees that the final result is different.\n",
    "\n",
    "With the checkpointing, it is possible to restore the last state in a straighforward manner and obtain strictly the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433dc229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 4.591223585995676e-09\n",
      "        x: [ 1.000e+00  9.999e-01]\n",
      "      nit: 17\n",
      "      jac: [ 1.789e-03 -9.432e-04]\n",
      "     nfev: 21\n",
      "     njev: 21\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "res_checkpoint = minimize_lbfgsb(\n",
    "    x0=res_3_fun.x,\n",
    "    fun=rosenbrock,\n",
    "    jac=rosenbrock_grad,\n",
    "    bounds=bounds,\n",
    "    ftol=1e-5,\n",
    "    gtol=1e-5,\n",
    "    checkpoint=res_3_fun,\n",
    ")\n",
    "print(res_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587f8fa",
   "metadata": {},
   "source": [
    "Note that this time, we keep track of `nfev` and `njev`. In addition, the results is strictly the same as minimizing the function in\n",
    "a single run. This can be pretty useful if computing the objective function and its gradient is expensive but one\n",
    "is not so sure about what stopping criteria to use. TODO: add something about use case for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d42ff",
   "metadata": {},
   "source": [
    "### ‚ú® Callback\n",
    "\n",
    "Our implementation of L-BFGS-B allows to use several standard stop criteria:\n",
    "\n",
    "- The absolute value of the objective function\n",
    "- The change in objective function value between two iterations.\n",
    "- And the norm of the objective function gradient.\n",
    "- The maximum number of iterations.\n",
    "- The maximum number of objective function calls.\n",
    "\n",
    "The callback mechanism allows to enhance these possibilities and define custom stopping criteria.\n",
    "For example, one can redefine the criterion based on the number of objective function evaluations\n",
    "(`maxfun`). If the algorithm should stop, the callback must return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1da69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: STOP: USER CALLBACK\n",
      "  success: True\n",
      "   status: 2\n",
      "      fun: 0.10848643847945114\n",
      "        x: [ 7.025e-01  4.794e-01]\n",
      "      nit: 8\n",
      "      jac: [ 3.379e+00 -2.828e+00]\n",
      "     nfev: 10\n",
      "     njev: 10\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import OptimizeResult\n",
    "\n",
    "\n",
    "def my_custom_callback(\n",
    "    xk: np.typing.NDArray[np.float64], state: OptimizeResult\n",
    ") -> bool:\n",
    "    # if the objective function has been called 10 times or more => stop\n",
    "    if state.nfev >= 10:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "res_callback = minimize_lbfgsb(\n",
    "    x0=x0,\n",
    "    fun=rosenbrock,\n",
    "    jac=rosenbrock_grad,\n",
    "    bounds=bounds,\n",
    "    ftol=1e-5,\n",
    "    gtol=1e-5,\n",
    "    callback=my_custom_callback,\n",
    ")\n",
    "print(res_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49d808",
   "metadata": {},
   "source": [
    "### ‚ú® Cost function update\n",
    "\n",
    "Function to update the gradient sequence. This allows changing the objective function definition on the fly.\n",
    "In the first place this functionality is dedicated to regularized problems for which the\n",
    "regularization weight is computed while optimizing the cost function. In order to get a\n",
    "Hessian matching the new definition of `fun`, the gradient sequence must be updated.\n",
    "\n",
    "```bash\n",
    "    ``update_fun_def(x, f0, f0_old, grad, x_deque, grad_deque)\n",
    "    -> f0, f0_old, grad, updated grad_deque``\n",
    "```\n",
    "\n",
    "üèóÔ∏è Complete example with supporting paper coming Q1 2026."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
